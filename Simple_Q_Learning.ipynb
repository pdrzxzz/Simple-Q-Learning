{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pdrzxzz/Simple-Q-Learning/blob/main/Simple_Q_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MATRIZ DE TRANSIÇÃO DE ESTADOS (PARA SIMULAÇÃO DO AGENTE)**"
      ],
      "metadata": {
        "id": "DkJEA8Ek_kbJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mb5mLKvCyHaw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Cria uma matriz 11x11 preenchida com zeros\n",
        "T_up = np.zeros((11, 11))\n",
        "\n",
        "# Preenche a matriz com as probabilidades de transição\n",
        "T_up[0, 3] = 0.1\n",
        "T_up[0, 0] = 0.1\n",
        "T_up[0, 1] = 0.8\n",
        "T_up[1, 1] = 0.2\n",
        "T_up[1, 2] = 0.8\n",
        "T_up[2, 4] = 0.1\n",
        "T_up[2, 2] = 0.9\n",
        "T_up[3, 5] = 0.1\n",
        "T_up[3, 3] = 0.8\n",
        "T_up[3, 0] = 0.1\n",
        "T_up[4, 7] = 0.1\n",
        "T_up[4, 2] = 0.1\n",
        "T_up[4, 4] = 0.8\n",
        "T_up[5, 8] = 0.1\n",
        "T_up[5, 3] = 0.1\n",
        "T_up[5, 6] = 0.8\n",
        "T_up[6, 9] = 0.1\n",
        "T_up[6, 6] = 0.1\n",
        "T_up[6, 7] = 0.8\n",
        "T_up[7, 10] = 0.1\n",
        "T_up[7, 4] = 0.1\n",
        "T_up[7, 7] = 0.8\n",
        "T_up[8, 8] = 0.1\n",
        "T_up[8, 5] = 0.1\n",
        "T_up[8, 9] = 0.8\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# MATRIZ DE TRANSIÇÃO PARA A AÇÃO DOWN\n",
        "T_down = np.zeros((11, 11))\n",
        "T_down[0, 0] = 0.9\n",
        "T_down[0, 3] = 0.1\n",
        "T_down[1, 0] = 0.8\n",
        "T_down[1, 1] = 0.2\n",
        "T_down[2, 1] = 0.8\n",
        "T_down[2, 2] = 0.1\n",
        "T_down[2, 4] = 0.1\n",
        "T_down[3, 0] = 0.1\n",
        "T_down[3, 5] = 0.1\n",
        "T_down[3, 3] = 0.8\n",
        "T_down[4, 4] = 0.8\n",
        "T_down[4, 2] = 0.1\n",
        "T_down[4, 7] = 0.1\n",
        "T_down[5, 5] = 0.8\n",
        "T_down[5, 3] = 0.1\n",
        "T_down[5, 8] = 0.1\n",
        "T_down[6, 5] = 0.8\n",
        "T_down[6, 6] = 0.1\n",
        "T_down[6, 9] = 0.1\n",
        "T_down[7, 6] = 0.8\n",
        "T_down[7, 4] = 0.1\n",
        "T_down[7, 10] = 0.1\n",
        "T_down[8, 8] = 0.9\n",
        "T_down[8, 5] = 0.1\n",
        "\n",
        "# MATRIZ DE TRANSIÇÃO PARA A AÇÃO LEFT\n",
        "T_left = np.zeros((11, 11))\n",
        "T_left[0, 0] = 0.9\n",
        "T_left[0, 1] = 0.1\n",
        "T_left[1, 0] = 0.1\n",
        "T_left[1, 2] = 0.1\n",
        "T_left[1, 1] = 0.8\n",
        "T_left[2, 2] = 0.9\n",
        "T_left[2, 1] = 0.1\n",
        "T_left[3, 0] = 0.8\n",
        "T_left[3, 3] = 0.2\n",
        "T_left[4, 2] = 0.8\n",
        "T_left[4, 4] = 0.2\n",
        "T_left[5, 3] = 0.8\n",
        "T_left[5, 5] = 0.1\n",
        "T_left[5, 6] = 0.1\n",
        "T_left[6, 6] = 0.8\n",
        "T_left[6, 5] = 0.1\n",
        "T_left[6, 7] = 0.1\n",
        "T_left[7, 4] = 0.8\n",
        "T_left[7, 7] = 0.1\n",
        "T_left[7, 6] = 0.1\n",
        "T_left[8, 5] = 0.8\n",
        "T_left[8, 8] = 0.1\n",
        "T_left[8, 9] = 0.1\n",
        "\n",
        "# MATRIZ DE TRANSIÇÃO PARA A AÇÃO RIGHT\n",
        "T_right = np.zeros((11, 11))\n",
        "T_right[0, 3] = 0.8\n",
        "T_right[0, 0] = 0.1\n",
        "T_right[0, 1] = 0.1\n",
        "T_right[1, 1] = 0.8\n",
        "T_right[1, 0] = 0.1\n",
        "T_right[1, 2] = 0.1\n",
        "T_right[2, 4] = 0.8\n",
        "T_right[2, 2] = 0.1\n",
        "T_right[2, 1] = 0.1\n",
        "T_right[3, 5] = 0.8\n",
        "T_right[3, 3] = 0.2\n",
        "T_right[4, 7] = 0.8\n",
        "T_right[4, 4] = 0.2\n",
        "T_right[5, 8] = 0.9\n",
        "T_right[5, 5] = 0.1\n",
        "T_right[5, 6] = 0.1\n",
        "T_right[6, 9] = 0.8\n",
        "T_right[6, 5] = 0.1\n",
        "T_right[6, 7] = 0.1\n",
        "T_right[7, 10] = 0.8\n",
        "T_right[7, 6] = 0.1\n",
        "T_right[7, 7] = 0.1\n",
        "T_right[8, 9] = 0.1\n",
        "T_right[8, 8] = 0.9\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FUNÇÕES AUXILIARES**"
      ],
      "metadata": {
        "id": "Kfe8LApI_vkZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para simular o resultado estocástico de aplicar uma ação ao estado atual\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def calc_action_result(state, transition_state):\n",
        "\n",
        "    # Obtém os índices dos estados candidatos (com probabilidade > 0)\n",
        "    cand_states = np.where(transition_state != 0)[0]\n",
        "    prod_cand_states = transition_state[cand_states]\n",
        "\n",
        "    # Ordena as probabilidades e obtém os índices ordenados\n",
        "    sorted_indices = np.argsort(prod_cand_states)\n",
        "    cand_states_sort = cand_states[sorted_indices]\n",
        "    prod_cand_states_sort = prod_cand_states[sorted_indices]\n",
        "\n",
        "    # Constrói a roleta acumulada\n",
        "    roleta = np.cumsum(prod_cand_states_sort)\n",
        "\n",
        "    # Gera número aleatório uniforme entre 0 e 1\n",
        "    r = np.random.uniform()\n",
        "\n",
        "    # Encontra o primeiro índice da roleta onde a probabilidade acumulada excede r\n",
        "    ind = np.where(roleta > r)[0]\n",
        "\n",
        "    return cand_states_sort[ind[0]]\n",
        "\n"
      ],
      "metadata": {
        "id": "iDx5op7SzTta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def choose_best_action(q_matrix, state):\n",
        "    \"\"\"\n",
        "    Função que retorna o índice da melhor ação (com maior valor Q)\n",
        "    para o estado atual.\n",
        "    \"\"\"\n",
        "    act = np.argmax(q_matrix[state])  # retorna índice (0-based)\n",
        "    return act\n"
      ],
      "metadata": {
        "id": "OtPWKXcJHKJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regra de Aprendizagem"
      ],
      "metadata": {
        "id": "CIJFfIWIGt5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def q_update(state, action, next_state, rw, q_matrix, alpha, gamma):\n",
        "    \"\"\"\n",
        "    Atualiza a estimativa Q para o par (estado, ação) com base na recompensa observada\n",
        "    e na estimativa de valor futuro.\n",
        "\n",
        "    Parâmetros:\n",
        "        state: estado atual (índice inteiro)\n",
        "        action: ação tomada (índice inteiro)\n",
        "        next_state: próximo estado (índice inteiro)\n",
        "        rw: vetor de recompensas por estado\n",
        "        q_matrix: matriz Q (numpy array 2D)\n",
        "        alpha: taxa de aprendizado\n",
        "        gamma: fator de desconto\n",
        "\n",
        "    Retorna:\n",
        "        Novo valor Q para o par (state, action)\n",
        "    \"\"\"\n",
        "    estimate_q = rw[state] + gamma * np.max(q_matrix[next_state, :])\n",
        "    q_value = q_matrix[state, action] + alpha * (estimate_q - q_matrix[state, action])\n",
        "    return q_value\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fcNs-mjB0tzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para executar a política aprendida e registrar a recompensa total acumulada\n",
        "\n",
        "def simulate_policy(q_matrix, rw):\n",
        "\n",
        "  r_total = 0\n",
        "\n",
        "  state = 0  # estado inicial\n",
        "\n",
        "  terminal = True\n",
        "\n",
        "  while terminal:\n",
        "\n",
        "      # Escolher ação com base na política aprendida\n",
        "      action_trial = choose_best_action(q_matrix, state)\n",
        "\n",
        "      # Selecionar a matriz de transição correspondente à ação\n",
        "      if action_trial == 0:\n",
        "          transition_state = T_up[state]\n",
        "      elif action_trial == 1:\n",
        "          transition_state = T_down[state]\n",
        "      elif action_trial == 2:\n",
        "          transition_state = T_left[state]\n",
        "      elif action_trial == 3:\n",
        "          transition_state = T_right[state]\n",
        "\n",
        "      # Aplicar a ação e observar o próximo estado\n",
        "      next_state = calc_action_result(state, transition_state)\n",
        "\n",
        "      print(f\"{state} {actions_names[action_trial]} {next_state}\")\n",
        "\n",
        "      # Acumular recompensa\n",
        "      r_total += rw[next_state]\n",
        "\n",
        "      # Atualizar estado\n",
        "      state = next_state\n",
        "\n",
        "      # Verificar se é estado terminal\n",
        "      if state == 9 or state == 10:  # estados terminais 9 e 10 em Python\n",
        "        terminal = False\n",
        "\n",
        "  # Resultado total acumulado\n",
        "  return r_total\n"
      ],
      "metadata": {
        "id": "VIZRpAPOHafv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função auxiliar para imprimir a política gerada a partir da matrix q\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def print_policy(q_matrix, actions):\n",
        "\n",
        "  # policy: índice da ação com maior valor Q em cada estado (max.col equivalente)\n",
        "  policy = np.argmax(q_matrix, axis=1)  # retorna array com índice da melhor ação por estado\n",
        "\n",
        "  # Em Python, índice começa em 0, então ajustamos os índices usados para extrair elementos do policy:\n",
        "  s1 = \" \".join([actions[policy[2]], actions[policy[4]], actions[policy[7]], \"+1\"])\n",
        "  s2 = \" \".join([actions[policy[1]], \"*\", actions[policy[6]], \"-1\"])\n",
        "  s3 = \" \".join([actions[policy[0]], actions[policy[3]], actions[policy[5]], actions[policy[8]]])\n",
        "\n",
        "  print(\"\\n\", s1, \"\\n\", s2, \"\\n\", s3)"
      ],
      "metadata": {
        "id": "E-udm4hd-IVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INICIALIZAÇÃO**"
      ],
      "metadata": {
        "id": "tNV1iIrr2h3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Nomes das ações (para referência, se quiser usar como rótulos)\n",
        "actions_names = [\"UP\", \"DW\", \"LF\", \"RG\"]\n",
        "\n",
        "# Inicialização da matriz Q (11 estados x 4 ações)\n",
        "q_matrix = np.zeros((11, 4))\n",
        "\n",
        "# Estados terminais (indexando a partir de 0 -- terminais 9 e 10 em Python)\n",
        "q_matrix[9, :] = -1    # Estado 10 (índice 9)\n",
        "q_matrix[10, :] = 1    # Estado 11 (índice 10)\n",
        "\n",
        "# Matriz de contagem de visitas para cada par (estado, ação) – opcional para controle\n",
        "N_matrix = np.zeros((11, 4))\n",
        "\n",
        "# Hiperparâmetros\n",
        "alpha = 0.2\n",
        "gamma = 0.5\n",
        "\n",
        "# Vetor de recompensas por estado\n",
        "rw = np.full(11, -0.04)\n",
        "rw[9] = -1     # Estado 10\n",
        "rw[10] = 1     # Estado 11\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UNO3jjVB2Vna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6TwWk5fN6RL",
        "outputId": "fa3a28a4-e0b3-4997-e1ff-4511efb02074"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.06643533, -0.07057427, -0.06820756, -0.06858258],\n",
              "       [-0.04756321, -0.06931443, -0.06214667, -0.05849136],\n",
              "       [-0.03655621, -0.05779945, -0.0383112 , -0.0048018 ],\n",
              "       [-0.06731169, -0.06663796, -0.07071311, -0.05930554],\n",
              "       [ 0.05490116,  0.04853853, -0.02152811,  0.12577683],\n",
              "       [-0.02745247, -0.05950791, -0.05808859, -0.06976389],\n",
              "       [ 0.0351199 , -0.2286377 , -0.02366054, -0.44397881],\n",
              "       [ 0.14132844,  0.07973733,  0.02223779,  0.43514816],\n",
              "       [-0.37509833, -0.06849387, -0.08696782, -0.22560932],\n",
              "       [-1.        , -1.        , -1.        , -1.        ],\n",
              "       [ 1.        ,  1.        ,  1.        ,  1.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kj200v-oSxBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**APRENDIZADO**"
      ],
      "metadata": {
        "id": "UHL836G8_eav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cada execução do loop representa uma exploração do ambiente\n",
        "# a partir do estado inicial, escolhendo ações aleatórias\n",
        "# até alcançar um estado terminal.\n",
        "import pandas as pd\n",
        "\n",
        "# número de trajetórias\n",
        "\n",
        "T = 5\n",
        "\n",
        "for i in range(T):\n",
        "\n",
        "    print(f\"Trajetoria {i}\")\n",
        "    state = 0\n",
        "\n",
        "    terminal = True\n",
        "\n",
        "    while terminal:\n",
        "        # Escolher uma ação aleatória\n",
        "        action_trial = np.random.choice([0, 1, 2, 3])  # 0: UP, 1: DW, 2: LF, 3: RG\n",
        "\n",
        "        # Incrementar a contagem de visitas\n",
        "        N_matrix[state, action_trial] += 1\n",
        "\n",
        "        # Selecionar a matriz de transição correspondente à ação escolhida\n",
        "        if action_trial == 0:\n",
        "            transition_state = T_up[state, :]\n",
        "        elif action_trial == 1:\n",
        "            transition_state = T_down[state, :]\n",
        "        elif action_trial == 2:\n",
        "            transition_state = T_left[state, :]\n",
        "        elif action_trial == 3:\n",
        "            transition_state = T_right[state, :]\n",
        "\n",
        "        # Simular a ação e obter o próximo estado\n",
        "        next_state = calc_action_result(state, transition_state)\n",
        "\n",
        "        # Mostrar a transição --- comentado\n",
        "        # print(f\"{state} {actions_names[action_trial]} {next_state}\")\n",
        "\n",
        "        # Atualizar o valor Q\n",
        "        q_matrix[state, action_trial] = q_update(\n",
        "            state, action_trial, next_state, rw, q_matrix, alpha, gamma\n",
        "        )\n",
        "\n",
        "        # Atualizar o estado atual\n",
        "        state = next_state\n",
        "\n",
        "        # Verifica se é estado terminal (estado 10 ou 11 → índices 9 ou 10)\n",
        "        if state == 9 or state == 10:\n",
        "          terminal = False\n",
        "\n",
        "#    print(\"\")  # linha em branco entre episódios\n",
        "\n",
        "\n",
        "q_matrix_df = pd.DataFrame(q_matrix, columns=actions_names)\n",
        "\n",
        "print(q_matrix_df)\n",
        "\n",
        "print_policy(q_matrix, actions_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBLCF1wL2mCx",
        "outputId": "cf8f4e79-6cde-4466-86f8-c4176b82b964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trajetoria 0\n",
            "Trajetoria 1\n",
            "Trajetoria 2\n",
            "Trajetoria 3\n",
            "Trajetoria 4\n",
            "          UP        DW        LF        RG\n",
            "0  -0.064818 -0.071926 -0.071031 -0.067935\n",
            "1  -0.045792 -0.069707 -0.061293 -0.062170\n",
            "2  -0.039726 -0.053371 -0.043046  0.000567\n",
            "3  -0.068458 -0.068962 -0.071912 -0.058713\n",
            "4   0.033690  0.054786 -0.029204  0.131254\n",
            "5  -0.031638 -0.059508 -0.054196 -0.061016\n",
            "6   0.035120 -0.266315 -0.023416 -0.363735\n",
            "7   0.141328  0.042953  0.022916  0.440119\n",
            "8  -0.408079 -0.068494 -0.081255 -0.225609\n",
            "9  -1.000000 -1.000000 -1.000000 -1.000000\n",
            "10  1.000000  1.000000  1.000000  1.000000\n",
            "\n",
            " RG RG RG +1 \n",
            " UP * UP -1 \n",
            " UP RG UP DW\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "N_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-b2ClnocRT29",
        "outputId": "18293bfc-1e2a-4c2e-ae68-02660754eaf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[158., 173., 167., 155.],\n",
              "       [141., 151., 120., 139.],\n",
              "       [114., 106.,  97.,  76.],\n",
              "       [108., 103., 100.,  86.],\n",
              "       [ 39.,  52.,  48.,  52.],\n",
              "       [ 45.,  57.,  54.,  50.],\n",
              "       [ 17.,  23.,  21.,  20.],\n",
              "       [ 23.,  18.,  24.,  20.],\n",
              "       [ 23.,  22.,  27.,  15.],\n",
              "       [  0.,   0.,   0.,   0.],\n",
              "       [  0.,   0.,   0.,   0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SIMULANDO A POLÍTICA APRENDIDA**"
      ],
      "metadata": {
        "id": "pzR_HTeiIS4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulando a execução da política a partir do estado inicial ate o estado final alcançado e registrado a recompensa total obtiga pelo agente\n",
        "\n",
        "r_total = simulate_policy(q_matrix, rw)\n",
        "\n",
        "print(r_total)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6qhRRu3G9xA",
        "outputId": "af0de3d3-4550-4caf-f03b-f66f6b7ebe51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 UP 1\n",
            "1 UP 2\n",
            "2 RG 4\n",
            "4 RG 7\n",
            "7 RG 6\n",
            "6 UP 9\n",
            "-1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obs.: exercícios para praticar"
      ],
      "metadata": {
        "id": "kjNCZWcJ8Y4S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ITEM 1:**"
      ],
      "metadata": {
        "id": "d4QucP5OL7tT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Treinamento:** Execute o Q Learning variando os parâmetros de alpha e gamma, com cinco opções de valores para cada parâmetro. Obs.: vc pode usar como critério de parada o número de 30 trajetórias ou algum critério de convergência (sobre a matrix Q). Execute o Q Learning 10 vezes para cada combinação de alpha e gamma.\n",
        "\n",
        "*  **Avaliação:** Para avaliar cada política aprendida, simule a execução da política por várias vezes (por exemplo, 10 vezes) e registre a média da recompensa total recebida.\n",
        "\n",
        "* Defina assim a melhor configuração de alpha e gamma avaliada.\n",
        "\n",
        "Tabela de resultados (exemplo):\n",
        "\n",
        "Alpha | Gamma | Recompensa Total (Media das 100 execuções) | Desvio\n",
        "\n",
        "0.2   | 0.1   |       xxx                                   |  yyy\n",
        "\n",
        "0.4   | 0.3   |       xxx                                   |  yyy\n",
        "\n",
        "0.6   | 0.5   |       xxx                                   |  yyy\n",
        "etc...\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "K3hSXomRKuVY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ITEM 2:**"
      ],
      "metadata": {
        "id": "bYXYjKonMCFq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Implemente duas estratégias de exploração de estados, como eps-greedy, Boltzman ou UCB. Execute e avalie o Q Learning com cada uma das estratégias.\n",
        "\n",
        "* Obs.: para simplificar fixe os valores de alpha e gamma obtidos no item anterior, mas se tiver tempo pode realizar experimentos adicionais. Novamente vc pode usar o número de 30 trajetórias como critério de parada.\n",
        "\n",
        "* Obs.: A qualidade da estratégia de exploração depende de parâmetros. Avalie opções diferentes de valores."
      ],
      "metadata": {
        "id": "vFiQbZzDMFiH"
      }
    }
  ]
}