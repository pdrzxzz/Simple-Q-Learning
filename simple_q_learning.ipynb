{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/pdrzxzz/Simple-Q-Learning/blob/main/Simple_Q_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DkJEA8Ek_kbJ"
   },
   "source": [
    "# Transition Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4UTV9xFb4PMG"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = mpimg.imread('../imgs/problem.png')\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Can use this under approach in a notebook environment.\n",
    "# from IPython.display import Image\n",
    "# Image(\"../imgs/problem.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mb5mLKvCyHaw",
    "outputId": "1677a411-3fc2-4654-d5c1-2c1681e720af"
   },
   "outputs": [],
   "source": [
    "# Create empty transition matrices for each action\n",
    "T = {\n",
    "    \"up\":    np.zeros((11, 11)),\n",
    "    \"down\":  np.zeros((11, 11)),\n",
    "    \"left\":  np.zeros((11, 11)),\n",
    "    \"right\": np.zeros((11, 11))\n",
    "}\n",
    "\n",
    "# Fill the matrices with the transition probabilites\n",
    "\n",
    "# === UP ===\n",
    "T[\"up\"][0, 3] = 0.1 # For example, if current_state = 0 and going up, the chance of going to state 3 is 0.1,\n",
    "T[\"up\"][0, 0] = 0.1 # to state 0 is also 0.1,\n",
    "T[\"up\"][0, 1] = 0.8 # and to state 1 is 0.8 (total 1.0).\n",
    "T[\"up\"][1, 1] = 0.2\n",
    "T[\"up\"][1, 2] = 0.8\n",
    "T[\"up\"][2, 4] = 0.1\n",
    "T[\"up\"][2, 2] = 0.9\n",
    "T[\"up\"][3, 5] = 0.1\n",
    "T[\"up\"][3, 3] = 0.8\n",
    "T[\"up\"][3, 0] = 0.1\n",
    "T[\"up\"][4, 7] = 0.1\n",
    "T[\"up\"][4, 2] = 0.1\n",
    "T[\"up\"][4, 4] = 0.8\n",
    "T[\"up\"][5, 8] = 0.1\n",
    "T[\"up\"][5, 3] = 0.1\n",
    "T[\"up\"][5, 6] = 0.8\n",
    "T[\"up\"][6, 9] = 0.1\n",
    "T[\"up\"][6, 6] = 0.1\n",
    "T[\"up\"][6, 7] = 0.8\n",
    "T[\"up\"][7, 10] = 0.1\n",
    "T[\"up\"][7, 4] = 0.1\n",
    "T[\"up\"][7, 7] = 0.8\n",
    "T[\"up\"][8, 8] = 0.1\n",
    "T[\"up\"][8, 5] = 0.1\n",
    "T[\"up\"][8, 9] = 0.8\n",
    "\n",
    "# === DOWN ===\n",
    "T[\"down\"][0, 0] = 0.9\n",
    "T[\"down\"][0, 3] = 0.1\n",
    "T[\"down\"][1, 0] = 0.8\n",
    "T[\"down\"][1, 1] = 0.2\n",
    "T[\"down\"][2, 1] = 0.8\n",
    "T[\"down\"][2, 2] = 0.1\n",
    "T[\"down\"][2, 4] = 0.1\n",
    "T[\"down\"][3, 0] = 0.1\n",
    "T[\"down\"][3, 5] = 0.1\n",
    "T[\"down\"][3, 3] = 0.8\n",
    "T[\"down\"][4, 4] = 0.8\n",
    "T[\"down\"][4, 2] = 0.1\n",
    "T[\"down\"][4, 7] = 0.1\n",
    "T[\"down\"][5, 5] = 0.8\n",
    "T[\"down\"][5, 3] = 0.1\n",
    "T[\"down\"][5, 8] = 0.1\n",
    "T[\"down\"][6, 5] = 0.8\n",
    "T[\"down\"][6, 6] = 0.1\n",
    "T[\"down\"][6, 9] = 0.1\n",
    "T[\"down\"][7, 6] = 0.8\n",
    "T[\"down\"][7, 4] = 0.1\n",
    "T[\"down\"][7, 10] = 0.1\n",
    "T[\"down\"][8, 8] = 0.9\n",
    "T[\"down\"][8, 5] = 0.1\n",
    "\n",
    "# === LEFT ===\n",
    "T[\"left\"][0, 0] = 0.9\n",
    "T[\"left\"][0, 1] = 0.1\n",
    "T[\"left\"][1, 0] = 0.1\n",
    "T[\"left\"][1, 2] = 0.1\n",
    "T[\"left\"][1, 1] = 0.8\n",
    "T[\"left\"][2, 2] = 0.9\n",
    "T[\"left\"][2, 1] = 0.1\n",
    "T[\"left\"][3, 0] = 0.8\n",
    "T[\"left\"][3, 3] = 0.2\n",
    "T[\"left\"][4, 2] = 0.8\n",
    "T[\"left\"][4, 4] = 0.2\n",
    "T[\"left\"][5, 3] = 0.8\n",
    "T[\"left\"][5, 5] = 0.1\n",
    "T[\"left\"][5, 6] = 0.1\n",
    "T[\"left\"][6, 6] = 0.8\n",
    "T[\"left\"][6, 5] = 0.1\n",
    "T[\"left\"][6, 7] = 0.1\n",
    "T[\"left\"][7, 4] = 0.8\n",
    "T[\"left\"][7, 7] = 0.1\n",
    "T[\"left\"][7, 6] = 0.1\n",
    "T[\"left\"][8, 5] = 0.8\n",
    "T[\"left\"][8, 8] = 0.1\n",
    "T[\"left\"][8, 9] = 0.1\n",
    "\n",
    "# === RIGHT ===\n",
    "T[\"right\"][0, 3] = 0.8\n",
    "T[\"right\"][0, 0] = 0.1\n",
    "T[\"right\"][0, 1] = 0.1\n",
    "T[\"right\"][1, 1] = 0.8\n",
    "T[\"right\"][1, 0] = 0.1\n",
    "T[\"right\"][1, 2] = 0.1\n",
    "T[\"right\"][2, 4] = 0.8\n",
    "T[\"right\"][2, 2] = 0.1\n",
    "T[\"right\"][2, 1] = 0.1\n",
    "T[\"right\"][3, 5] = 0.8\n",
    "T[\"right\"][3, 3] = 0.2\n",
    "T[\"right\"][4, 7] = 0.8\n",
    "T[\"right\"][4, 4] = 0.2\n",
    "T[\"right\"][5, 8] = 0.8\n",
    "T[\"right\"][5, 5] = 0.1\n",
    "T[\"right\"][5, 6] = 0.1\n",
    "T[\"right\"][6, 9] = 0.8\n",
    "T[\"right\"][6, 5] = 0.1\n",
    "T[\"right\"][6, 7] = 0.1\n",
    "T[\"right\"][7, 10] = 0.8\n",
    "T[\"right\"][7, 6] = 0.1\n",
    "T[\"right\"][7, 7] = 0.1\n",
    "T[\"right\"][8, 9] = 0.1\n",
    "T[\"right\"][8, 8] = 0.9\n",
    "\n",
    "# Check if each row in each transition matrix sums exactly to 1.0 (if it has transitions)\n",
    "error_found = False\n",
    "\n",
    "for action_name, matrix in T.items():\n",
    "    for state_index, row in enumerate(matrix):\n",
    "        total_prob = row.sum()\n",
    "\n",
    "        # Ignore rows with no outgoing transitions\n",
    "        if total_prob == 0:\n",
    "            continue\n",
    "\n",
    "        # Check for exact match with 1.0\n",
    "        if total_prob != 1.0:\n",
    "            print(f\"⚠️ Warning: action='{action_name}', state={state_index}, total probability = {total_prob}\")\n",
    "            error_found = True\n",
    "\n",
    "# If no errors were found, print success message\n",
    "if not error_found:\n",
    "    print(\"✅ All transition probabilities are correct.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kfe8LApI_vkZ"
   },
   "source": [
    "# AUXILIARY FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10xhbpXL-_ox"
   },
   "source": [
    "## Stochastic Transition Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iDx5op7SzTta"
   },
   "outputs": [],
   "source": [
    "def calc_action_result(transition_state):\n",
    "    \"\"\"\n",
    "    Given a transition probability vector for all possible next states,\n",
    "    returns the next state chosen stochastically based on the distribution.\n",
    "\n",
    "    Parameters:\n",
    "        transition_state (ndarray): 1D array with probabilities for each state.\n",
    "\n",
    "    Returns:\n",
    "        int: The index of the selected next state.\n",
    "    \"\"\"\n",
    "    # Get the indices of candidate states (with probability > 0)\n",
    "    possible_states = np.where(transition_state != 0)[0]  # e.g. [0, 0.1, 0.9, 0] → [1, 2]\n",
    "\n",
    "    # Get the probabilities of the candidate states\n",
    "    probabilities = transition_state[possible_states]     # e.g. [0.1, 0.9]\n",
    "\n",
    "    # Sort the probabilities and get the sorted indices\n",
    "    sorted_indices = np.argsort(probabilities)\n",
    "    sorted_states = possible_states[sorted_indices]\n",
    "    sorted_probabilities = probabilities[sorted_indices]\n",
    "\n",
    "    # Build the cumulative probability distribution (roulette)\n",
    "    roulette = np.cumsum(sorted_probabilities)\n",
    "\n",
    "    # Draw a number between 0 and 1\n",
    "    random_value = np.random.uniform()\n",
    "\n",
    "    # Find the first index in the cumulative distribution where the accumulated probability exceeds random_value\n",
    "    chosen_index = np.where(roulette > random_value)[0][0]\n",
    "\n",
    "    # Return the chosen state\n",
    "    return sorted_states[chosen_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OtPWKXcJHKJ_"
   },
   "outputs": [],
   "source": [
    "def choose_best_action(q_table, current_state):\n",
    "    \"\"\"\n",
    "    Returns the index of the best action (highest Q-value) for the given state.\n",
    "\n",
    "    Parameters:\n",
    "        q_table (ndarray): 2D array where q_table[state][action] holds the Q-value.\n",
    "        current_state (int): Index of the current state.\n",
    "\n",
    "    Returns:\n",
    "        int: Index of the best known action (0 = UP, 1 = DOWN, 2 = LEFT, 3 = RIGHT).\n",
    "    \"\"\"\n",
    "    return np.argmax(q_table[current_state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIJFfIWIGt5Z"
   },
   "source": [
    "Regra de Aprendizagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fcNs-mjB0tzA"
   },
   "outputs": [],
   "source": [
    "def q_update(q_table, current_state, next_state_probs, action, reward, learning_rate, discount_factor):\n",
    "    \"\"\"\n",
    "    Updates the Q-table using the Q-learning formula. This function implements the core Q-learning\n",
    "    update step, incorporating the observed reward and estimated future rewards.\n",
    "\n",
    "    Parameters:\n",
    "        q_table (ndarray): The Q-table, a 2D array where q_table[state][action] holds the\n",
    "                           estimated maximum future rewards for taking action in a given state.\n",
    "        current_state (int): The index of the state the agent is currently in.\n",
    "        next_state_probs (ndarray): A 1D array representing the probability distribution\n",
    "                                    over possible next states after taking the chosen action\n",
    "                                    from the current state.\n",
    "        action (int): The index of the action taken by the agent in the current state.\n",
    "        reward (float): The immediate reward received after taking the action and transitioning\n",
    "                        to the next state.\n",
    "        learning_rate (float): Alpha (α), a value between 0 and 1. It determines how much the\n",
    "                               new information (the difference between the estimated future reward\n",
    "                               and the current Q-value) impacts the current Q-value. A higher\n",
    "                               alpha means faster learning but can lead to instability.\n",
    "        discount_factor (float): Gamma (γ), a value between 0 and 1. It discounts the value\n",
    "                                 of future rewards. A higher gamma means the agent considers\n",
    "                                 future rewards more important, while a lower gamma focuses\n",
    "                                 more on immediate rewards.\n",
    "\n",
    "    Returns:\n",
    "        None. The q_table is updated in place.\n",
    "    \"\"\"\n",
    "    # Simulate the result of taking the action: choose next state probabilistically\n",
    "    next_state = calc_action_result(next_state_probs)\n",
    "\n",
    "    # Get the best Q-value for the next state\n",
    "    best_future_q = np.max(q_table[next_state])\n",
    "\n",
    "    # Apply the Q-learning update rule\n",
    "    q_table[current_state, action] += learning_rate * (\n",
    "        reward + discount_factor * best_future_q - q_table[current_state, action]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VIZRpAPOHafv"
   },
   "outputs": [],
   "source": [
    "def simulate_policy(q_table, reward_vector):\n",
    "    \"\"\"\n",
    "    Executes the learned policy starting from the initial state (0)\n",
    "    and returns the total accumulated reward until reaching a terminal state.\n",
    "\n",
    "    Parameters:\n",
    "        q_table (ndarray): Q-values table with shape [state][action].\n",
    "        reward_vector (ndarray): Reward for each state (size = number of states).\n",
    "\n",
    "    Returns:\n",
    "        float: Total reward accumulated by following the greedy policy.\n",
    "    \"\"\"\n",
    "\n",
    "    total_reward = 0  # Initial reward accumulator\n",
    "    state = 0         # Start from state 0\n",
    "    terminal = False  # Flag to check if episode ended\n",
    "\n",
    "    while not terminal:\n",
    "        # Select the best action in the current state (greedy policy)\n",
    "        action_index = choose_best_action(q_table, state)\n",
    "\n",
    "        # Map the action index to its name\n",
    "        action_name = [\"up\", \"down\", \"left\", \"right\"][action_index]\n",
    "\n",
    "        # Get transition probabilities for the chosen action\n",
    "        transition_probabilities = T[action_name][state]\n",
    "\n",
    "        # Sample next state stochastically\n",
    "        next_state = calc_action_result(transition_probabilities)\n",
    "\n",
    "        # Log current step\n",
    "        print(f\"{state} {action_name} {next_state}\")\n",
    "\n",
    "        # Accumulate reward of resulting state\n",
    "        total_reward += reward_vector[next_state]\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "        # Check for terminal condition (states 9 or 10)\n",
    "        if state in [9, 10]:\n",
    "            terminal = True\n",
    "\n",
    "    return total_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-udm4hd-IVZ"
   },
   "outputs": [],
   "source": [
    "def print_policy(q_table, actions):\n",
    "    \"\"\"\n",
    "    Displays the derived policy from the Q-table using a DataFrame grid layout.\n",
    "\n",
    "    Parameters:\n",
    "        q_table (ndarray): Q-values table with shape [state][action].\n",
    "        actions (list of str): List of action names indexed by action number.\n",
    "    Returns:\n",
    "      None, renders a visual layout.\n",
    "    \"\"\"\n",
    "\n",
    "    # Mapping action names to symbols\n",
    "    symbol_map = {\n",
    "        \"up\": \"↑\",\n",
    "        \"down\": \"↓\",\n",
    "        \"left\": \"←\",\n",
    "        \"right\": \"→\"\n",
    "    }\n",
    "\n",
    "    # Derive best actions (indices) and map to symbols\n",
    "    best_action_indices = np.argmax(q_table, axis=1)\n",
    "    best_action_names = [actions[i] for i in best_action_indices]\n",
    "    best_action_symbols = [symbol_map[name] for name in best_action_names]\n",
    "\n",
    "    # Manually define layout based on your fixed environment\n",
    "    # Use '' for invalid states, and manually insert special states\n",
    "    layout = [\n",
    "        [best_action_symbols[2], best_action_symbols[4], best_action_symbols[7], \"+1\"],\n",
    "        [best_action_symbols[1], \"X\",                    best_action_symbols[6], \"-1\"],\n",
    "        [best_action_symbols[0], best_action_symbols[3], best_action_symbols[5], best_action_symbols[8]]\n",
    "    ]\n",
    "\n",
    "    df = pd.DataFrame(layout)\n",
    "\n",
    "    print(\"\\nPolicy layout:\")\n",
    "    print(df.to_string(index=False, header=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNV1iIrr2h3B"
   },
   "source": [
    "**INICIALIZAÇÃO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UNO3jjVB2Vna"
   },
   "outputs": [],
   "source": [
    "# Define action names in a consistent order\n",
    "ACTION_NAMES = [\"up\", \"down\", \"left\", \"right\"]\n",
    "\n",
    "# Number of states and actions\n",
    "NUM_STATES = 11\n",
    "NUM_ACTIONS = len(ACTION_NAMES)\n",
    "\n",
    "# Initialize Q-table with zeros\n",
    "q_table = np.zeros((NUM_STATES, NUM_ACTIONS))\n",
    "\n",
    "# Define terminal states and set their Q-values explicitly\n",
    "TERMINAL_STATES = {9: -1, 10: 1}\n",
    "for state, value in TERMINAL_STATES.items():\n",
    "    q_table[state, :] = value\n",
    "\n",
    "# Initialize visit counts for state-action pairs (optional, useful for analysis)\n",
    "visit_counts = np.zeros((NUM_STATES, NUM_ACTIONS))\n",
    "\n",
    "# Hyperparameters for Q-learning\n",
    "ALPHA = 0.2      # Learning rate\n",
    "GAMMA = 0.5      # Discount factor\n",
    "\n",
    "# Define reward vector per state with default penalty for non-terminal states\n",
    "reward_vector = np.full(NUM_STATES, -0.04)\n",
    "for state, reward in TERMINAL_STATES.items():\n",
    "    reward_vector[state] = reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHL836G8_eav"
   },
   "source": [
    "**APRENDIZADO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vBLCF1wL2mCx",
    "outputId": "3ce8ad66-c875-4087-8cf1-16a9df7c03c6"
   },
   "outputs": [],
   "source": [
    "# Number of trajectories for environment exploration\n",
    "num_trajectories = 5000\n",
    "\n",
    "for i in range(num_trajectories):\n",
    "    state = 0\n",
    "    terminal = False\n",
    "\n",
    "    while not terminal:\n",
    "        # Choose a random action from the available actions\n",
    "        action_idx = np.random.choice(len(ACTION_NAMES))\n",
    "        visit_counts[state, action_idx] += 1  # Update visit count\n",
    "\n",
    "        action_name = ACTION_NAMES[action_idx]\n",
    "        transition_probs = T[action_name][state]\n",
    "\n",
    "        # Update Q-table based on action and transition\n",
    "        q_update(q_table, state, transition_probs, action_idx, reward_vector[state], ALPHA, GAMMA)\n",
    "\n",
    "        # Choose the next state based on transition probabilities\n",
    "        next_state = calc_action_result(transition_probs)\n",
    "\n",
    "        # Update current state\n",
    "        state = next_state\n",
    "\n",
    "        # Check if a terminal state has been reached\n",
    "        if state in TERMINAL_STATES:\n",
    "            terminal = True\n",
    "\n",
    "print(f\"{num_trajectories} trajectories done!\")\n",
    "\n",
    "# Display final Q-table formatted with pandas\n",
    "print(\"\\nFinal Q-table:\")\n",
    "print(pd.DataFrame(q_table, columns=ACTION_NAMES))\n",
    "\n",
    "# Display the policy derived from the Q-table\n",
    "print_policy(q_table, ACTION_NAMES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-b2ClnocRT29",
    "outputId": "80287541-0a35-4241-ca00-0b1e39c83348"
   },
   "outputs": [],
   "source": [
    "print(pd.DataFrame(visit_counts, columns=ACTION_NAMES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pzR_HTeiIS4T"
   },
   "source": [
    "**SIMULANDO A POLÍTICA APRENDIDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D6qhRRu3G9xA",
    "outputId": "0f9d1d1a-f451-40c8-e4ac-ec3114e90987"
   },
   "outputs": [],
   "source": [
    "# Simulando a execução da política a partir do estado inicial ate o estado final alcançado e registrado a recompensa total obtiga pelo agente\n",
    "\n",
    "r_total = simulate_policy(q_table, reward_vector)\n",
    "\n",
    "print(\"Recompensa total:\", r_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kjNCZWcJ8Y4S"
   },
   "source": [
    "Obs.: exercícios para praticar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4QucP5OL7tT"
   },
   "source": [
    "# **ITEM 1:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K3hSXomRKuVY"
   },
   "source": [
    "* **Treinamento:** Execute o Q Learning variando os parâmetros de alpha e gamma, com cinco opções de valores para cada parâmetro. Obs.: vc pode usar como critério de parada o número de 30 trajetórias ou algum critério de convergência (sobre a matrix Q). Execute o Q Learning 10 vezes para cada combinação de alpha e gamma.\n",
    "\n",
    "*  **Avaliação:** Para avaliar cada política aprendida, simule a execução da política por várias vezes (por exemplo, 10 vezes) e registre a média da recompensa total recebida.\n",
    "\n",
    "* Defina assim a melhor configuração de alpha e gamma avaliada.\n",
    "\n",
    "Tabela de resultados (exemplo):\n",
    "\n",
    "Alpha | Gamma | Recompensa Total (Media das 100 execuções) | Desvio\n",
    "\n",
    "0.2   | 0.1   |       xxx                                   |  yyy\n",
    "\n",
    "0.4   | 0.3   |       xxx                                   |  yyy\n",
    "\n",
    "0.6   | 0.5   |       xxx                                   |  yyy\n",
    "etc...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bYXYjKonMCFq"
   },
   "source": [
    "# **ITEM 2:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFiQbZzDMFiH"
   },
   "source": [
    "* Implemente duas estratégias de exploração de estados, como eps-greedy, Boltzman ou UCB. Execute e avalie o Q Learning com cada uma das estratégias.\n",
    "\n",
    "* Obs.: para simplificar fixe os valores de alpha e gamma obtidos no item anterior, mas se tiver tempo pode realizar experimentos adicionais. Novamente vc pode usar o número de 30 trajetórias como critério de parada.\n",
    "\n",
    "* Obs.: A qualidade da estratégia de exploração depende de parâmetros. Avalie opções diferentes de valores."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
